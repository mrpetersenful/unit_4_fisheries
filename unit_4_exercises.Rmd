---
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.width=6, fig.asp = 0.618, collapse=TRUE) 
```

### In-class exercises
### Unit 4: Fisheries

***

### Exercise 1.1

Let’s imagine you had a month of scuba survey data where each row was a different fish that was observed on a rocky reef. The fish survey data includes the fish’s common name, size, date, reef site and observation ID. Then you have a second data frame that has encylopedia-type data downloaded from fishbase.org with common name, genus, species, trophic level and maximum length. If your goal was to add the genus and species information to your survey data and then count the total number of each species observed at a reef, what join would you use? What variable would you join the data frames by? What would happen if there were multiple rows in your fishbase data frame corresponding to the same common name (perhaps one row included a max length estimate pulled from Miller et al. and another row included a max length pulled from Garcia et al.)? Could that mess up your scuba survey analysis?

ANSWER: I would use `left_join(survey_data, fish_base_data, by="common_name")` so that I retain all of my survey data and add in the fish base data wherever it is available. Then I would use dim() to compare the number of rows in my survey data before and after the join. If my merged data was longer, I would either: 

1) If I really cared about max length, I would comb through the fish base data frame and remove the duplicate rows based on which max length study was more relevant to my analysis. 
2) If I only wanted the genus and species info anyways, I would use distinct(observation_id) to remove duplicate rows of observations - then check again to make sure that the number of rows in my survey data frame is equal to the number of rows before the join function!

***

### Exercise 1.2

When the invertebrate survey is in the orignial (wide) format, use ggplot to create a scatter plot where the quadrat_id is along the x-axis and the count data is along the y-axis. Give each different taxon a different point color. Now create the same exact plot using the survey data in the long format.

If you wanted to build a linear model to predict the number of barnacles in a quadrat as a function of the number of other invertebrates that were found in the quadrat (i.e. chitons and mussels), what table shape would you need, long or wide?  ### WIDE ###

```{r, eval=FALSE}
# We need to write a new geom_point() line for each x/y combo (i.e. each species)
ggplot(data=survey) +
  geom_point(aes(x=quadrat_id, y=barnacle_n), color='red' ) +
  geom_point(aes(x=quadrat_id, y=chiton_n), color='green' ) +
  geom_point(aes(x=quadrat_id, y=mussel_n), color='blue' ) 

# Look how easy it is to plot when your data is in the long format
ggplot(aes(x=quadrat_id, y=counts, color=taxon), data=long) +
  geom_point()
```

*** 

### Exercise 2.1

Create the same time series that shows the # of stocks that have collapsed (historically) divided by the total number of stocks that are tracked in the dataset. However, show this plot separately for EACH region. You may need to create a new data frame that counts the number of stocks tracked in each region, then join that new data frame to the collapse_ts data frame to calculate your ratios.

```{r, eval=FALSE}
# Create a time series of # of stocks ever collapsed / total stocks for EACH region
n_stock_assessments = collapse %>%
  distinct(stockid, .keep_all=TRUE) %>%
  group_by(region) %>%
  summarize(n_stocks_per_region = n())

collapse_ts = collapse_yr %>%
  left_join(n_stock_assessments) %>%
  group_by(region, n_stocks_per_region) %>%
  count(first_collapse_yr) %>%
  mutate(cum_first_collapse_yr = cumsum(n),
         ratio_collapsed_yet = cum_first_collapse_yr/n_stocks_per_region)

ggplot(data = collapse_ts, aes(x=first_collapse_yr, y=ratio_collapsed_yet)) +
  geom_line() +
  facet_wrap(~region)
```

***

### Exercise 3.1

Try running the same poisson model predicting the number of years that a stock is collapsed as a function of the ratio of overfished years and the ratio of low stock years. This time, only include data from the Atlantic Ocean region. Test the poisson model to see if overdispersion is an issue. If it is a problem, refit the model as a quasipoisson. What are some advantages and disadvantages to  breaking the data into distinct regions?

```{r, eval=FALSE}
table(collapse_summary_zero_trunc_region$region)
# Get subset of data on US East Coast
collapse_summary_zero_trunc_region = collapse_summary_zero_trunc %>%
  left_join(stock %>% select(stockid, region))

# region=="Atlantic Ocean"
atlantic_collapse = collapse_summary_zero_trunc_region %>% filter(region=="Atlantic Ocean")

# Fit the poisson model
model_p_atl = glm(yrs_collapsed ~ ratio_yrs_overfished + ratio_yrs_low_stock , offset(log(yrs_data)), data=atlantic_collapse, family="poisson") 
summary(model_p_atl)

# Is there overdispersion?
AER::dispersiontest(model_p_atl)$p.value < 0.05 # TRUE = overdispersed; FALSE = NOT overdispersed

newdata = data.frame(ratio_yrs_overfished = seq(from=0,to=1,by=0.1),
                     ratio_yrs_low_stock = median(atlantic_collapse$ratio_yrs_low_stock))
model_p_atl_predict = predict(model_p_atl, type="response", newdata = newdata, se.fit=TRUE)

# Organize predictions into a tidy table
predictions = cbind(newdata, model_p_atl_predict)

# Plot predictions and SE ribbon
ggplot() +
  geom_line(aes(x=ratio_yrs_overfished, y=fit), data=predictions) +
  geom_ribbon( aes(x=ratio_yrs_overfished, ymin = fit-se.fit, ymax = fit+se.fit), fill="darkgrey", alpha = .5, data=predictions) +
  geom_point(aes(x=ratio_yrs_overfished, y=yrs_collapsed), data=atlantic_collapse) +
  ylab("# years stock was collapsed") +
  theme_bw()
```

When you model the Atlantic Ocean stock data on their own, you don't have a problem with overdisperion and ratio_yrs_overfished is a significant explanatory variable. When a stock is more chronically overfished, it spends more time in the collapsed state. Notably, there are only 

***
