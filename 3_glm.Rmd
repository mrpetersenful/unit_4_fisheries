---
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.width=6, fig.asp = 0.618, collapse=TRUE) 
```

### Unit 4: Fisheries
#### Lesson 3: Generalized linear models
#### New skills: glm(family="binomial"), glm(family="poisson"), AER::dispersiontest, glm(family="quasipoisson") 

***

### Exploring fisheries data with GLMs

In this lesson we will continue to use the RAM Legacy Database: 

https://www.ramlegacy.org/

```{r}
load('data/RAMLDB v4.491/DB Files With Assessment Data/R Data/DBdata[asmt][v4.491].RData')
```

***

First I'm going to import the code for the tables we built in the last class, such as fish_catch_max_assess and collapse. That way I can use the data manipulation work we've already done as a basis for running our models today. I just put all of the code used to generate these tables in their own R script, and now I use the `source()` function to run that R script and put all of the associated variables in my environment. You could also just copy and paste the code we wrote in the last class at the top of today's R script.

```{r, results="hide"}
source('build_collapse_table.R')
```

### Logistic regression

What features of a stock make it more (or less) likely to experience a collapse? I find this question fascinating, and I'd love to model the chance of collapse as a function of habitat type, geographic region, primary country that fishes that stock, fishing methods and age at sexual maturity. Unfortunately, the RAM data doesn't have a lot of that background info for most stocks (see bioparams table). We do consistently have `region`, i.e. the geographical location of the stock, so let's see if `region` is a significant explanatory variable for predicting the stock's collapse. 

Since collapse is a yes/no, TRUE/FALSE, 0/1, binomial type of variable, we should use logistic regression.

![](doc/logistic_regression.jpg){width=50%}

Logistic regression can be fit with the `glm()` function using the `binomial` link. After the model is fit, model predictions can be made with the `predict()` function just like we did with linear regressions. 

Logistic regression does not require model residuals to be normally distributed or variance to be constant. AIC can be used for model comparison, i.e. to determine whether the inclusion of an additional independent variable constitutes a significantly improved model. McFadden's Pseudo-$R^2$ can be used to assess fitness, where McFadden claims that a Pseudo-$R^2$ between 0.2 and 0.4 represents an "excellent fit". 


```{r}
# Remove time series information and collapse data to "has this stock EVER collapsed?"
model_data = collapse %>%
  group_by(stockid, region) %>%
  summarize(ever_collapsed = any(ever_collapsed)) %>%
  ungroup()
glimpse(model_data)

# Run a logistic regression
model_l = glm(ever_collapsed ~ region, data = model_data, family = "binomial")
summary(model_l)

# Make predictions on the probability of a stock collapse by region
regions = model_data %>% distinct(region)
model_l_predict = predict(model_l, newdata=regions, type="response", se.fit=TRUE)

# Organize predictions into a tidy table
collapse_region_predictions = data.frame(region = regions,
                                         predictions = model_l_predict$fit,
                                         se = model_l_predict$se.fit)

# Plot predictions and SE bars
ggplot(aes(x=region, y=predictions, fill=region), data=collapse_region_predictions) +
  geom_bar(stat="identity") +
  geom_errorbar(aes(ymin=predictions-se, ymax=predictions+se), width=0.2) +
  coord_flip() +
  theme(legend.position = "none")

# Calculate McFadden Pseudo-R^2: 0.2-0.4 = "Excellent fit"
# library(pscl)
# pscl::pR2(model_l)["McFadden"]
```

We have statistically significant, negative coefficients for the Mediterranean-Black Sea stocks and the West Africa stocks - meaning these stocks are not likely to collapse. The Canada East and West Coast stock groups are almost significant at $\alpha = 0.05$, and since those coefficients are positive that indicates that stocks are more likely to collapse. There are no examples of collapsed stocks in the Pacific Ocean and Indian Ocean regions.

Logistic regression can be a good tool for any type of binomial analysis, including binomial categorical variables (like sex), probabilities (including demographic or state space transition probabilities), or presence / absence occurrence data. Use it. Love it.

### Poisson model

The poisson regression is usually the best choice for modeling count data: discrete data with non-negative integer values. The poisson model can also be applied to rate data, as in the count of an event per unit of time, space, etc. The divisor of your rate data (i.e. the amount of time, space, etc. sampled) will be treated as an "offset" variable in the model. In ecology, poisson regression is appropriate for counting individuals in a survey. For example, fish counts along a transect may need to be offset by transect length if the length isn't standard in the survey. However, count of barnacles wouldn't require an offset term if the same size quadrat is always used. Number of events can also be modeled with poisson regression, such as the number of rainy days per month, with an offset for the total number of days in the month.

There are two snags that you can run into with a poisson model: zero inflation and overdispersion. 

Zero inflation can be problematic when there is an excessive amount of zeros in your count data. Often, the mechanisms that govern zero-counts may be different from the mechanisms that govern counts of >=1. For example, did you count zero of a certain fish species because the true number in that habitat is pretty low, or becuase you are in the completely wrong habitat? If you were an omniscient modeler, you would separate out the habitats that are "impossible" to find that species, and then run the model on the remaining data where it IS possible to find that species. However, it's more likely that you don't know enough to make that distinction (that's why you are conducting these surveys!). So if you have zero-inflated data, you'll have to consider removing the zeros and just explicitly modeling the positive count data, or designing a model that treats the zeros carefully (see below for more information). 

Overdispersion occurs when the observed variance is much higher than the mean. After running a poisson model, you should test for overdispersion (I use `AER::dispersiontest()`). If overdispersion is present, then you should switch to a different distributional family like a `quasipoisson` (where the variance is assumed to be a linear function of the mean) or a `negative binomial` (where the variance is assumed to be a quadratic function of the mean).

#### Model the period of time that a stock is collapsed

In the fisheries unit, I was hoping to demonstrate a poisson model where I predicted fish catch (i.e. the count of fish caught) with an offset for effort. CPUE (Catch Per Unit Effort) is a ubiquitous metric in fishery research. Unfortunately, there is very little effort data in the RAM dataset, so I decided to go a different route.

We can use a poisson distribution to model the count of years that a stock spends in the collapsed state. Is the length of time that a stock spends in the "collapsed" state dependent on how frequently it is overfished or how often its biomass is below $B_{MSY}$?

```{r}
# U is fishing pressure (often fishing mortalities) and B is biomass
# U / U_MSY and B / B_MSY?
# Calculate ratio of years a stock is overfished and stock biomass < B_MSY
u_summary = timeseries_values_views %>% 
  left_join(stock, by=c("stockid","stocklong")) %>%
  filter(!is.na(UdivUmsypref),
         !is.na(BdivBmsypref)) %>%
  group_by(stockid) %>%
  summarize(yrs_data = n(),
            ratio_yrs_overfished = sum(UdivUmsypref > 1)/yrs_data,
            ratio_yrs_low_stock = sum(BdivBmsypref < 1)/yrs_data) %>%
  select(-yrs_data)

# Count num years each stock is collapsed; join with counts of overfished-years and low-stock-years
collapse_summary = collapse %>%
  group_by(stockid, stocklong, region) %>%
  summarize(yrs_data = n(),
            yrs_collapsed = sum(current_collapse)) %>%
  left_join(u_summary, by="stockid")

# Do we have zero-inflation?
table(collapse_summary$yrs_collapsed)

# Create a zero-truncated data set to demonstrate poisson model
collapse_summary_zero_trunc = collapse_summary %>% filter(yrs_collapsed>0)

# Build poisson model
model_p = glm(yrs_collapsed ~ ratio_yrs_overfished + ratio_yrs_low_stock, offset(log(yrs_data)), data=collapse_summary_zero_trunc, family="poisson") 
summary(model_p)

# Do we have overdispersion?
AER::dispersiontest(model_p)$p.value < 0.05 # TRUE = overdispersed; FALSE = NOT overdispersed

# Address overdispersion with a quasipoisson or negative binomial model
model_qp = glm(yrs_collapsed ~ ratio_yrs_overfished + ratio_yrs_low_stock , offset(log(yrs_data)), data=collapse_summary_zero_trunc, family="quasipoisson") 
summary(model_qp)

# Make predictions on the time period a stock spends collapsed 
# as a function of low stock year rates with overfishing rates set to the observed median
newdata = data.frame(ratio_yrs_low_stock = seq(from=0,to=1,by=0.1),
                     ratio_yrs_overfished = median(collapse_summary_zero_trunc$ratio_yrs_overfished, na.rm=TRUE))
model_qp_predict = predict(model_qp, type="response", newdata = newdata, se.fit=TRUE)

# Organize predictions into a tidy table
collapse_time_predictions = cbind(newdata,
                                  data.frame(predictions = model_qp_predict$fit,
                                             se = model_qp_predict$se.fit))

# Plot predictions and SE ribbon
ggplot() +
  geom_line(aes(x=ratio_yrs_low_stock, y=predictions), data=collapse_time_predictions) +
  geom_ribbon( aes(x=ratio_yrs_low_stock, ymin = predictions-se, ymax = predictions+se), fill="darkgrey", alpha = .5, data=collapse_time_predictions) +
  geom_point(aes(x=ratio_yrs_low_stock, y=yrs_collapsed), data=collapse_summary_zero_trunc) +
  theme_bw()

# What fishery has been collapsed for 90 years??
collapse_summary %>% filter(yrs_collapsed > 75)  # Georges Bank Halibut!!

```

The count of years that a stock was in a collapsed state had tons of zeros, and the zero-inflation would make a problem for a poisson model fit. We dealt with this by removing the zeros, but we must keep in mind that this fundamentally changes the analysis to: "Out of the stocks that have experienced a collapse at some point, what drives the length of time they spend collapsed?". This is still an interesting (and related question) but the results should be presented in the right context. We also found overdispersion in the data, so we switched to a quasipoisson distribution.

The final `model_qp` shows that the number of years that a stock is overfished does NOT significantly drive the time spent in the collapsed state, but the length of time that a stocks biomass falls below $B_{MSY}$ IS a significant driver of the time spent in the collapsed state. We plotted the quasipoisson model fit across the range of possible low stock time periods, while holding the ratio of time spent in the overfished state constant at the observed median. The plot shows that the predicted time spent in the collapsed state goes from something like 10 to 20 years as the ratio of time spent in the low stock state moves from 0 to 1. 

***

### Exercise 3.1

Try running the same poisson model predicting the number of years that a stock is collapsed as a function of the ratio of overfished years and the ratio of low stock years. This time, only include data from the US East Coast. Test the poisson model to see if overdispersion is an issue. If it is a problem, refit the model as a quasipoisson. Do you think there is an advantage or a disadvantage to  breaking the data into distinct regions?

***


## EXTRA

```{r}

# U is fishing pressure (often fishing mortalities) and B is biomass
# U / U_MSY and B / B_MSY
u_dat = timeseries_values_views %>% 
  left_join(stock, by=c("stockid","stocklong")) %>%
  filter(!is.na(UdivUmsypref)) %>%
  filter(!is.na(BdivBmsypref))

# frequency of overfishing by region
overfished_count_by_region = u_dat %>%
  filter(year>=1950) %>%
  group_by(year, region) %>%
  summarize(n_stocks = n(), 
            overfished = sum(UdivUmsypref>1),   # Fishing pressure (U) > U_MSY
            underfished = sum(UdivUmsypref<=1), # Fishing pressure (U) <= U_MSY
            healthy_stock = sum(BdivBmsypref>=1),# Biomass (B) >= B_MSY
            unhealthy_stock = sum(BdivBmsypref<1))

ggplot() +
  geom_line(aes(x=year, y=overfished/n_stocks), data=overfished_count_by_region) +
  facet_wrap(~region)


### Bioparam data
# There are 828 unique stockid in bioparams, but only 61 trophic levels and only 298 habitats
length(unique(bioparams$stockid))
dim(bioparams %>% filter(bioid=="Habitat-Habitat"))
dim(bioparams %>% filter(bioid=="Trophiclevel-value"))

# Which bioparams show up most frequently across the different stocks?
bioparam_count = bioparams %>% group_by(bioid) %>% summarize(n=n()) %>% arrange(desc(n))
bioparam_count

# The habitat data is a mess
bioparams %>% 
  filter(bioid=="Habitat-Habitat") %>% 
  group_by(biovalue) %>%
  summarize(n = n())

# Clean it up
habitat_data = bioparams %>% 
  filter(bioid=="Habitat-Habitat") %>% 
  mutate(habitat = tolower(biovalue),
         habitat = ifelse(grepl( "demersal", habitat, fixed = TRUE),"demersal", habitat),
         habitat = ifelse(grepl( "pelagic", habitat, fixed = TRUE),"pelagic", habitat)) %>%
  filter(habitat %in% c("demersal", "pelagic")) %>%
  distinct(stockid, habitat)

```



### More Information

A good primer on logistic regression in R:
https://afit-r.github.io/logistic_regression

More info on zero inflation in poisson models:
You can formally check for zero-inflation with the `pscl::vuong()` test. To address zero-inflation, you have three options: 1) build a zero-truncated model, which means you remove the zeros from your data and run the model on your "presence" data only. This is a simple solution, but it throws away data, and may fundamentally change the question you are answering. 2) Build a two-part of "hurdle" model where you model presence/absence with one model, and model non-zero counts with a second model; then present the results of both models together. 3) Build a mixture model that attempts to distinguish true zeros from false zeros. For more info on addressing zero inflation in R: https://fukamilab.github.io/BIO202/04-C-zero-data.html

### Acknowledgements

Logistic regression figure was pulled from Data Camp:
https://www.datacamp.com/community/tutorials/logistic-regression-R

