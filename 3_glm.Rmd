---
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.width=6, fig.asp = 0.618, collapse=TRUE) 
```

### Unit 4: Fisheries
#### Lesson 3: Generalized linear models
#### New skills: glm(family="binomial"), glm(family="poisson"), AER::dispersiontest, glm(family="quasipoisson") 

***

### Exploring fisheries data with GLMs

In this lesson we will continue to use the RAM Legacy Database: 

https://www.ramlegacy.org/

```{r}
load('data/RAMLDB v4.491/DB Files With Assessment Data/R Data/DBdata[asmt][v4.491].RData')
```

***

First I'm going to import the code for the tables we built in the last class, such as fish_catch_max_assess and collapse. That way I can use the data manipulation work we've already done as a basis for running our models today. I just put all of the code used to generate these tables in their own R script, and now I use the `source()` function to run that R script and put all of the associated variables in my environment. You could also just copy and paste the code we wrote in the last class at the top of today's R script.

```{r, results="hide", message=FALSE}
source('build_collapse_table.R')
```

### Logistic regression

Logistic regression is used to model a binary dependent variable or a dependent variable that is bound between 0 and 1:

![](doc/logistic_regression.jpg){width=50%}

This is the logistic regression function where p(x) is our y variable, bounded between 0 and 1:

![](doc/logistic_regression_function.png){width=25%}

Although this equation doesn't look like a linear equation, a logistic function is a generalized linear model becuase the model is *linear in the predictors*, meaning that the model is fit to some linear combination of the x variables (the independent variables) and the model coefficients. That means that the equation can be re-arranged with x's all on one side of the equation looking like an ordinary linear regression model:

![](doc/logistic_regression_function_link.png){width=25%}

Logistic regression can be fit with the `glm()` function using the `binomial` family `glm` stands for Generalized Linear Models, and `binomial` implies that the y variable that we are predicting has 2 choices. After the model is fit, model predictions can be made with the `predict()` function just like we did with linear regressions. 

Logistic regression does not require model residuals to be normally distributed or variance to be constant. AIC can be used for model comparison, i.e. to determine whether the inclusion of an additional independent variable constitutes a significantly improved model. McFadden's Pseudo-$R^2$ can be used to assess fitness, where McFadden claims that a Pseudo-$R^2$ between 0.2 and 0.4 represents an "excellent fit". 

#### Which stocks have experienced a collapse?

What features of a stock make it more (or less) likely to experience a collapse? I find this question fascinating, and I'd love to model the chance of collapse as a function of habitat type, geographic region, primary country that fishes that stock, fishing methods and age at sexual maturity. Unfortunately, the RAM data doesn't have a lot of that background info for most stocks (see bioparams table). We do consistently have `region`, i.e. the geographical location of the stock, so let's see if `region` is a significant explanatory variable for predicting the stock's collapse. (Note that a chi-square test would also be an appropriate way to answer this question since we are comparing a categorical y variable against a single categorical x variable, but if we use a logistic regression model we could add more complexity with multiple x variables).

Since collapse is a yes/no, TRUE/FALSE, 0/1, binomial type of variable, we should use logistic regression.

```{r}
# Remove time series information and collapse data to "has this stock EVER collapsed?"
model_data = collapse %>%
  group_by(stockid) %>%
  summarize(ever_collapsed = any(current_collapse)) %>%
  ungroup() %>%
  left_join(stock)
head(model_data)
summary(model_data)

# Run a logistic regression
model_l = glm(ever_collapsed ~ region, data = model_data, family = "binomial")
summary(model_l)

# arrange region alphabetically to see that intercept represents "Atlantic Ocean" 
model_data %>% distinct(region) %>% arrange(region)
```

We have statistically significant, negative coefficients for the Mediterranean-Black Sea stocks and the West Africa stocks - meaning these stocks are less likely to collapse than the reference region, which is the Atlantic Ocean. There are no examples of collapsed stocks in the Pacific Ocean and Indian Ocean regions.

To create the prediction plot, we use `predict.glm` and generate model predictions of a stock's probability of ever experiencing a collapse for every region in the dataset. We generate predictions with `type="response"` because we want our predictions to be on the scale of the response (y) variable. That means we want predictions between 0 and 1, using this equation:

![](doc/logistic_regression_function.png){width=25%}

It is important to note that the default option for `predict.glm` is to generate predictions with `type="link"`, meaning that the predictions are generated on the scale of the linear predictors. In the case of a logistic regression, these predictions would be log-odds, and are calculated using this equation:

![](doc/logistic_regression_function_link.png){width=25%}

Since I want to plot the probability that a stock might experience a collapse for a given region, I'll use `type="response"`:

```{r}
# Make predictions on the probability of a stock collapse by region
regions = model_data %>% distinct(region)
model_l_predict = predict(model_l, newdata=regions, type="response", se.fit=TRUE)

# Organize predictions into a tidy table
collapse_region_predictions = cbind(regions, model_l_predict)

# Plot predictions and SE bars
ggplot(aes(x=region, y=fit, fill=region), data=collapse_region_predictions) +
  geom_bar(stat="identity", show.legend = FALSE) + # stat="count" is default (like histogram)
  geom_errorbar(aes(ymin=fit-se.fit, ymax=fit+se.fit), width=0.2) +
  coord_flip() +
  ylab("Probability of stock collapse")
  # theme(legend.position = "none") # another way to hide a legend

# Calculate McFadden Pseudo-R^2: 0.2-0.4 = "Excellent fit"
# library(pscl)
# pscl::pR2(model_l)["McFadden"]
```

Logistic regression can be a good tool for any type of binomial analysis, including binomial categorical variables (like male/female, pass/fail, yes/no), probabilities (including demographic or state space transition probabilities), or presence / absence occurrence data. Use it. Love it.

### Poisson model

The poisson regression is usually the best choice for modeling count data: discrete data with non-negative integer values. The poisson model can also be applied to rate data, as in the count of an event per unit of time, space, etc. The divisor of your rate data (i.e. the amount of time, space, etc. sampled) will be treated as an "offset" variable in the model. In ecology, poisson regression is appropriate for counting individuals in a survey. For example, fish counts along a transect may need to be offset by transect length if the length isn't standard in the survey. However, count of barnacles wouldn't require an offset term if the same size quadrat is always used. Number of events can also be modeled with poisson regression, such as the number of rainy days per month, with an offset for the total number of days in the month.

There are two snags that you can run into with a poisson model: zero inflation and overdispersion. 

Zero inflation can be problematic when there is an excessive amount of zeros in your count data. Often, the mechanisms that govern zero-counts may be different from the mechanisms that govern counts of >=1. For example, did you count zero of a certain fish species because the true number in that habitat is pretty low, or becuase you are in the completely wrong habitat? If you were an omniscient modeler, you would separate out the habitats that are "impossible" to find that species, and then run the model on the remaining data where it IS possible to find that species. However, it's more likely that you don't know enough to make that distinction (that's why you are conducting these surveys!). So if you have zero-inflated data, you'll have to consider removing the zeros and just explicitly modeling the positive count data, or designing a model that treats the zeros carefully (see below for more information). 

Overdispersion occurs when the observed variance in your count data is much higher than the mean. After running a poisson model, you should test for overdispersion (I use `AER::dispersiontest()`). If overdispersion is present, then you should switch to a different distributional family like a `quasipoisson` (where the variance is assumed to be a linear function of the mean) or a `negative binomial` (where the variance is assumed to be a quadratic function of the mean).

#### Model the period of time that a stock is collapsed

In the fisheries unit, I was hoping to demonstrate a poisson model where I predicted fish catch (i.e. the count of fish caught) with an offset for effort. CPUE (Catch Per Unit Effort) is a ubiquitous metric in fishery research. Unfortunately, there is very little effort data in the RAM dataset, so I decided to go a different route.

We can use a poisson distribution to model the count of years that a stock spends in the collapsed state. Is the length of time that a stock spends in the "collapsed" state dependent on how frequently it is overfished or how often its biomass is below $B_{MSY}$?

```{r}
# U is fishing pressure (often fishing mortalities) and B is biomass
# U / U_MSY is actual U relative to U at Max Sustainable Yield
# B / B_MSY is actual B relative to B at Max Sustainable Yield
# Calculate ratio of years a stock is overfished (U/U_MSY > 1) 
# and ratio of years stock biomass is too low (B / B_MSY < 1) 
u_summary = timeseries_values_views %>% 
  #left_join(stock, by=c("stockid","stocklong")) %>%
  filter(!is.na(UdivUmsypref),
         !is.na(BdivBmsypref)) %>%
  group_by(stockid, stocklong) %>%
  summarize(yrs_data = n(),
            ratio_yrs_overfished = sum(UdivUmsypref > 1)/yrs_data,
            ratio_yrs_low_stock = sum(BdivBmsypref < 1)/yrs_data) %>%
  select(-yrs_data)

# Count num years each stock is collapsed; join with counts of overfished-years and low-stock-years
collapse_summary = collapse %>%
  group_by(stockid) %>%
  summarize(yrs_data = n(),
            yrs_collapsed = sum(current_collapse)) %>%
  inner_join(u_summary, by="stockid") # only keep stocks that have collapse data AND u_summary data

# Do we have zero-inflation?
table(collapse_summary$yrs_collapsed)

# Create a zero-truncated data set to demonstrate poisson model
collapse_summary_zero_trunc = collapse_summary %>% filter(yrs_collapsed>0)

# Build poisson model
model_p = glm(yrs_collapsed ~ ratio_yrs_overfished + ratio_yrs_low_stock, offset(log(yrs_data)), data=collapse_summary_zero_trunc, family="poisson") 
summary(model_p)

# Do we have overdispersion?
# library(AER) # don't need to load library if using package::function() notation
AER::dispersiontest(model_p)$p.value < 0.05 # TRUE = overdispersed; FALSE = NOT overdispersed

# Address overdispersion with a quasipoisson or negative binomial model
model_qp = glm(yrs_collapsed ~ ratio_yrs_overfished + ratio_yrs_low_stock , offset(log(yrs_data)), data=collapse_summary_zero_trunc, family="quasipoisson") 
summary(model_qp)

# Make predictions on the time period a stock spends collapsed 
# as a function of low stock year rates with overfishing rates set to the observed median
newdata = data.frame(ratio_yrs_low_stock = seq(from=0,to=1,by=0.1),
                     ratio_yrs_overfished = median(collapse_summary_zero_trunc$ratio_yrs_overfished))
model_qp_predict = predict(model_qp, type="response", newdata = newdata, se.fit=TRUE)

# Organize predictions into a tidy table
collapse_time_predictions = cbind(newdata, model_qp_predict)

# Plot predictions and SE ribbon
ggplot() +
  geom_line(aes(x=ratio_yrs_low_stock, y=fit), data=collapse_time_predictions) +
  geom_ribbon( aes(x=ratio_yrs_low_stock, ymin = fit-se.fit, ymax = fit+se.fit), fill="darkgrey", alpha = .5, data=collapse_time_predictions) +
  geom_point(aes(x=ratio_yrs_low_stock, y=yrs_collapsed), data=collapse_summary_zero_trunc) +
  ylab("# years stock was collapsed") +
  theme_bw()

# What fishery has been collapsed for 90 years??
collapse_summary %>% filter(yrs_collapsed > 75)  # Georges Bank Halibut!!

```

The count of years that a stock was in a collapsed state had tons of zeros, and the zero-inflation would make a problem for a poisson model fit. We dealt with this by removing the zeros, but we must keep in mind that this fundamentally changes the analysis to: "Out of the stocks that have experienced a collapse at some point, what drives the length of time they spend collapsed?". This is still an interesting (and related question) but the results should be presented in the right context. We also found overdispersion in the data, which can lead to biased standard errors. We can address this by switching to a quasipoisson distribution.

The final `model_qp` shows that the number of years that a stock is overfished does NOT significantly drive the time spent in the collapsed state, but the length of time that a stocks biomass falls below $B_{MSY}$ IS a significant driver of the time spent in the collapsed state. We plotted the quasipoisson model fit across the range of possible low stock time periods, while holding the ratio of time spent in the overfished state constant at the observed median. The plot shows that the predicted time spent in the collapsed state goes from something like 10 to 20 years as the ratio of time spent in the low stock state moves from 0 to 1. 

***

### Exercise 3.1

Try running the same poisson model predicting the number of years that a stock is collapsed as a function of the ratio of overfished years and the ratio of low stock years. This time, only include data from the US East Coast. Test the poisson model to see if overdispersion is an issue. If it is a problem, refit the model as a quasipoisson. Do you think there is an advantage or a disadvantage to  breaking the data into distinct regions?

***

### More Information

A good primer on logistic regression in R:
https://afit-r.github.io/logistic_regression

More info on zero inflation in poisson models:
You can formally check for zero-inflation with the `pscl::vuong()` test. To address zero-inflation, you have three options: 1) build a zero-truncated model, which means you remove the zeros from your data and run the model on your "presence" data only. This is a simple solution, but it throws away data, and may fundamentally change the question you are answering. 2) Build a two-part of "hurdle" model where you model presence/absence with one model, and model non-zero counts with a second model; then present the results of both models together. 3) Build a mixture model that attempts to distinguish true zeros from false zeros. For more info on addressing zero inflation in R: https://fukamilab.github.io/BIO202/04-C-zero-data.html

### Acknowledgements

Logistic regression figure was pulled from Data Camp:
https://www.datacamp.com/community/tutorials/logistic-regression-R

